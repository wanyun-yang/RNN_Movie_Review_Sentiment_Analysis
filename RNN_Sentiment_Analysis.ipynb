{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Sentiment_Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPA+NcHQOkUekXUb4WCfOBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wanyun-yang/RNN_Movie_Review_Sentiment_Analysis/blob/main/RNN_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6lJL2hA65p4",
        "outputId": "c082ccce-8b01-48d2-b8fe-3f1df398f1f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import files, drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWO7kZPm20dl",
        "outputId": "66648a24-f349-47b0-842c-aeac51daabdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "pos_root_dir = './drive/My Drive/reviews/pos/'\n",
        "neg_root_dir = './drive/My Drive/reviews/neg/'\n",
        "pos_files = os.listdir(pos_root_dir)\n",
        "neg_files = os.listdir(neg_root_dir)\n",
        "\n",
        "max_features = 200\n",
        "\n",
        "corpus = []\n",
        "labels = []\n",
        "for i in range(500):\n",
        "  f = pos_files[i]\n",
        "  with open(pos_root_dir+f) as fh:\n",
        "    corpus.append(fh.read().replace('\\n',' '))\n",
        "    labels.append([1,0])\n",
        "for i in range(500):\n",
        "  f = neg_files[i]\n",
        "  with open(neg_root_dir+f) as fh:\n",
        "    corpus.append(fh.read().replace('\\n',' '))\n",
        "    labels.append([0,1])\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features = max_features, stop_words = 'english')\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "y = np.array(labels)\n",
        "print(X.shape,y.shape)\n",
        "print(X[0,5])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 200) (1000, 2)\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpGwhGGB0Uo_",
        "outputId": "d20b1317-3f26-49c2-bf85-62c1622459ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seq_length = -1\n",
        "\n",
        "word_tokenizer = vectorizer.build_tokenizer()\n",
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "doc_terms_list_train = [word_tokenizer(doc_str) for doc_str in corpus]\n",
        "docs = []\n",
        "for i in range(len(doc_terms_list_train)):\n",
        "  terms = []\n",
        "  for j in range(len(doc_terms_list_train[i])):\n",
        "    w = doc_terms_list_train[i][j]\n",
        "    if w in vocab:\n",
        "      terms.append(w)\n",
        "  if len(terms) > seq_length:\n",
        "    seq_length = len(terms)\n",
        "  docs.append(terms)\n",
        "\n",
        "datasets = np.zeros((X.shape[0],seq_length,max_features))\n",
        "\n",
        "for i in range(len(docs)):\n",
        "  # Padding\n",
        "  n_padding = seq_length - len(docs[i])\n",
        "\n",
        "  for j in range(len(docs[i])):\n",
        "    w = docs[i][j]\n",
        "    idx = vocab[w]\n",
        "    tfidf_val = X[i,idx]\n",
        "    datasets[i,j+n_padding,idx] = tfidf_val\n",
        "\n",
        "datasets = datasets.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "X_train,X_val,y_train,y_val = train_test_split(datasets, y, test_size=0.2, random_state=2020)\n",
        "print(X_train.shape,y_train.shape,X_val.shape,y_val.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(800, 312, 200) (800, 2) (200, 312, 200) (200, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhK2xGG3vfIv"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(X_train),torch.from_numpy(y_train))\n",
        "val_data = TensorDataset(torch.from_numpy(X_val),torch.from_numpy(y_val))\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_data,shuffle=True,batch_size=batch_size)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPYPONxosXbb",
        "outputId": "6f1a627d-0e51-4117-ef7e-6b05316e1f56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Model(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "# Define three types of layer.\n",
        "    self.rnn = nn.RNN(input_size,hidden_size,n_layers,batch_first=True)\n",
        "    self.fc1 = nn.Linear(hidden_size,output_size)\n",
        "    self.fc2 = nn.Linear(output_size,2)\n",
        "  \n",
        "  def forward(self,x,hidden):\n",
        "    batch_size = x.size()[0]\n",
        "\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "\n",
        "    rnn_out,hidden = self.rnn(x,hidden)\n",
        "    rnn_out = self.fc1(rnn_out)\n",
        "    last_out = rnn_out[:,-1,:].view(batch_size,-1)\n",
        "    out = F.softmax(self.fc2(last_out))\n",
        "\n",
        "    return out,hidden\n",
        "  \n",
        "  def init_hidden(self,batch_size):\n",
        "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda()\n",
        "    return hidden\n",
        "\n",
        "model = Model(200,32,256,2)   \n",
        "print(model) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (rnn): RNN(200, 256, num_layers=2, batch_first=True)\n",
            "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBAzPKzOuQXh",
        "outputId": "47b62c19-2e77-4b88-da48-b221051f0724",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 10\n",
        "lr = 1e-4\n",
        "counter = 0\n",
        "clip = 5\n",
        "\n",
        "#Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "  # Initialize hidden state\n",
        "  h = model.init_hidden(batch_size)\n",
        "\n",
        "  # Batch loop\n",
        "  for inputs,labels in train_loader:\n",
        "    counter +=1\n",
        "    inputs,labels = inputs.to(device),labels.to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    outputs,h = model(inputs,h)\n",
        "\n",
        "    loss = criterion(outputs,torch.max(labels,1)[1])\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm(model.parameters(),clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    #Validation loss\n",
        "    if counter % 10 ==0:\n",
        "      val_h = model.init_hidden(batch_size).cuda()\n",
        "      val_losses = []\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      for inputs,labels in val_loader:\n",
        "        inputs, labels = inputs.to(device),labels.to(device)\n",
        "        val_outputs, val_h = model(inputs,val_h)\n",
        "        val_loss = criterion(val_outputs,torch.max(labels,1)[1])\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      print('Epoch:{}/{}'.format(epoch+1,n_epochs),\n",
        "            'Batch:{}'.format(counter),\n",
        "            'Train Loss:{:.5f}'.format(loss.item()),\n",
        "            'Val Loss:{:.5f}'.format(np.mean(val_losses)))\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1/10 Batch:10 Train Loss:0.34564 Val Loss:0.76024\n",
            "Epoch:1/10 Batch:20 Train Loss:0.36415 Val Loss:0.77606\n",
            "Epoch:1/10 Batch:30 Train Loss:0.33272 Val Loss:0.76105\n",
            "Epoch:1/10 Batch:40 Train Loss:0.45860 Val Loss:0.77072\n",
            "Epoch:1/10 Batch:50 Train Loss:0.35602 Val Loss:0.79467\n",
            "Epoch:2/10 Batch:60 Train Loss:0.42307 Val Loss:0.77409\n",
            "Epoch:2/10 Batch:70 Train Loss:0.38102 Val Loss:0.78406\n",
            "Epoch:2/10 Batch:80 Train Loss:0.32051 Val Loss:0.77911\n",
            "Epoch:2/10 Batch:90 Train Loss:0.32899 Val Loss:0.77176\n",
            "Epoch:2/10 Batch:100 Train Loss:0.39577 Val Loss:0.78708\n",
            "Epoch:3/10 Batch:110 Train Loss:0.34775 Val Loss:0.78270\n",
            "Epoch:3/10 Batch:120 Train Loss:0.32256 Val Loss:0.76374\n",
            "Epoch:3/10 Batch:130 Train Loss:0.38029 Val Loss:0.79891\n",
            "Epoch:3/10 Batch:140 Train Loss:0.31961 Val Loss:0.79128\n",
            "Epoch:3/10 Batch:150 Train Loss:0.32689 Val Loss:0.78123\n",
            "Epoch:4/10 Batch:160 Train Loss:0.38726 Val Loss:0.77406\n",
            "Epoch:4/10 Batch:170 Train Loss:0.32208 Val Loss:0.77891\n",
            "Epoch:4/10 Batch:180 Train Loss:0.31482 Val Loss:0.78570\n",
            "Epoch:4/10 Batch:190 Train Loss:0.31618 Val Loss:0.78823\n",
            "Epoch:4/10 Batch:200 Train Loss:0.40515 Val Loss:0.79519\n",
            "Epoch:5/10 Batch:210 Train Loss:0.32693 Val Loss:0.78333\n",
            "Epoch:5/10 Batch:220 Train Loss:0.31549 Val Loss:0.80716\n",
            "Epoch:5/10 Batch:230 Train Loss:0.37918 Val Loss:0.77814\n",
            "Epoch:5/10 Batch:240 Train Loss:0.32344 Val Loss:0.79050\n",
            "Epoch:5/10 Batch:250 Train Loss:0.35092 Val Loss:0.80636\n",
            "Epoch:6/10 Batch:260 Train Loss:0.33040 Val Loss:0.79374\n",
            "Epoch:6/10 Batch:270 Train Loss:0.37439 Val Loss:0.81699\n",
            "Epoch:6/10 Batch:280 Train Loss:0.31456 Val Loss:0.80262\n",
            "Epoch:6/10 Batch:290 Train Loss:0.31402 Val Loss:0.79924\n",
            "Epoch:6/10 Batch:300 Train Loss:0.31618 Val Loss:0.80076\n",
            "Epoch:7/10 Batch:310 Train Loss:0.31415 Val Loss:0.80444\n",
            "Epoch:7/10 Batch:320 Train Loss:0.37639 Val Loss:0.81508\n",
            "Epoch:7/10 Batch:330 Train Loss:0.31578 Val Loss:0.80061\n",
            "Epoch:7/10 Batch:340 Train Loss:0.31524 Val Loss:0.79758\n",
            "Epoch:7/10 Batch:350 Train Loss:0.31370 Val Loss:0.79840\n",
            "Epoch:8/10 Batch:360 Train Loss:0.31409 Val Loss:0.79826\n",
            "Epoch:8/10 Batch:370 Train Loss:0.31417 Val Loss:0.79515\n",
            "Epoch:8/10 Batch:380 Train Loss:0.31505 Val Loss:0.78271\n",
            "Epoch:8/10 Batch:390 Train Loss:0.31380 Val Loss:0.79968\n",
            "Epoch:8/10 Batch:400 Train Loss:0.32397 Val Loss:0.78979\n",
            "Epoch:9/10 Batch:410 Train Loss:0.31415 Val Loss:0.79256\n",
            "Epoch:9/10 Batch:420 Train Loss:0.31476 Val Loss:0.81155\n",
            "Epoch:9/10 Batch:430 Train Loss:0.31689 Val Loss:0.82262\n",
            "Epoch:9/10 Batch:440 Train Loss:0.31606 Val Loss:0.80666\n",
            "Epoch:9/10 Batch:450 Train Loss:0.31355 Val Loss:0.79630\n",
            "Epoch:10/10 Batch:460 Train Loss:0.37614 Val Loss:0.80866\n",
            "Epoch:10/10 Batch:470 Train Loss:0.31404 Val Loss:0.80473\n",
            "Epoch:10/10 Batch:480 Train Loss:0.31341 Val Loss:0.80353\n",
            "Epoch:10/10 Batch:490 Train Loss:0.37690 Val Loss:0.80484\n",
            "Epoch:10/10 Batch:500 Train Loss:0.31380 Val Loss:0.80866\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}