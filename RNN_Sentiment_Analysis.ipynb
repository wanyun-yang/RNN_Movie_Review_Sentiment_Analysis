{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Sentiment_Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0tB8mlBJloguXBi0QjrPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wanyun-yang/RNN_Movie_Review_Sentiment_Analysis/blob/main/RNN_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6lJL2hA65p4",
        "outputId": "72f575fa-79ae-4362-f90a-a5af419f2925",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import files, drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWO7kZPm20dl",
        "outputId": "486a7e25-287a-4ac6-89c5-3e17871349ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "pos_root_dir = './drive/My Drive/reviews/pos/'\n",
        "neg_root_dir = './drive/My Drive/reviews/neg/'\n",
        "pos_files = os.listdir(pos_root_dir)\n",
        "neg_files = os.listdir(neg_root_dir)\n",
        "\n",
        "max_features = 200\n",
        "\n",
        "corpus = []\n",
        "labels = []\n",
        "for i in range(500):\n",
        "  f = pos_files[i]\n",
        "  with open(pos_root_dir+f) as fh:\n",
        "    corpus.append(fh.read().replace('\\n',' '))\n",
        "    labels.append([1,0])\n",
        "for i in range(500):\n",
        "  f = neg_files[i]\n",
        "  with open(neg_root_dir+f) as fh:\n",
        "    corpus.append(fh.read().replace('\\n',' '))\n",
        "    labels.append([0,1])\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features = max_features, stop_words = 'english')\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "y = np.array(labels)\n",
        "print(X.shape,y.shape)\n",
        "print(X[0,5])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 200) (1000, 2)\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpGwhGGB0Uo_",
        "outputId": "6debeeec-06ca-4b7a-8add-fe9447a7ed05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seq_length = -1\n",
        "\n",
        "word_tokenizer = vectorizer.build_tokenizer()\n",
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "doc_terms_list_train = [word_tokenizer(doc_str) for doc_str in corpus]\n",
        "docs = []\n",
        "for i in range(len(doc_terms_list_train)):\n",
        "  terms = []\n",
        "  for j in range(len(doc_terms_list_train[i])):\n",
        "    w = doc_terms_list_train[i][j]\n",
        "    if w in vocab:\n",
        "      terms.append(w)\n",
        "  if len(terms) > seq_length:\n",
        "    seq_length = len(terms)\n",
        "  docs.append(terms)\n",
        "\n",
        "datasets = np.zeros((X.shape[0],seq_length,max_features))\n",
        "\n",
        "for i in range(len(docs)):\n",
        "  # Padding\n",
        "  n_padding = seq_length - len(docs[i])\n",
        "\n",
        "  for j in range(len(docs[i])):\n",
        "    w = docs[i][j]\n",
        "    idx = vocab[w]\n",
        "    tfidf_val = X[i,idx]\n",
        "    datasets[i,j+n_padding,idx] = tfidf_val\n",
        "\n",
        "datasets = datasets.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "X_train,X_val,y_train,y_val = train_test_split(datasets, y, test_size=0.2, random_state=2020)\n",
        "print(X_train.shape,y_train.shape,X_val.shape,y_val.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(800, 312, 200) (800, 2) (200, 312, 200) (200, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhK2xGG3vfIv"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(X_train),torch.from_numpy(y_train))\n",
        "val_data = TensorDataset(torch.from_numpy(X_val),torch.from_numpy(y_val))\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_data,shuffle=True,batch_size=batch_size)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPYPONxosXbb",
        "outputId": "3d8f1cfe-5575-4f55-d119-f57195710371",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Model(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "# Define three types of layer.\n",
        "    self.rnn = nn.RNN(input_size,hidden_size,n_layers,batch_first=True)\n",
        "    self.fc1 = nn.Linear(hidden_size,output_size)\n",
        "    self.fc2 = nn.Linear(output_size,2)\n",
        "  \n",
        "  def forward(self,x,hidden):\n",
        "    batch_size = x.size()[0]\n",
        "\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "\n",
        "    rnn_out,hidden = self.rnn(x,hidden)\n",
        "    rnn_out = self.fc1(rnn_out)\n",
        "    last_out = rnn_out[:,-1,:].view(batch_size,-1)\n",
        "    out = F.softmax(self.fc2(last_out))\n",
        "\n",
        "    return out,hidden\n",
        "  \n",
        "  def init_hidden(self,batch_size):\n",
        "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda()\n",
        "    return hidden\n",
        "\n",
        "model = Model(200,32,256,2)   \n",
        "print(model) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (rnn): RNN(200, 256, num_layers=2, batch_first=True)\n",
            "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBAzPKzOuQXh"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 10\n",
        "lr = 1e-4\n",
        "counter = 0\n",
        "clip = 5\n",
        "\n",
        "#Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "  # Initialize hidden state\n",
        "  h = model.init_hidden(batch_size)\n",
        "\n",
        "  # Batch loop\n",
        "  for inputs,labels in train_loader:\n",
        "    counter +=1\n",
        "    inputs,labels = inputs.to(device),labels.to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    output,h = model(inputs,h)\n",
        "\n",
        "    loss = criterion(outputs,torch.max(labels,1)[1])\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm(model.parameters(),clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    #Validation loss\n",
        "    if counter% 10 ==0:\n",
        "      val_h = model.init_hidden(batch_size).cuda()\n",
        "      val_losses = []\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      for inputs,labels in val_loader:\n",
        "        inputs, labels = inputs.to(device),labels.to(device)\n",
        "        val_outputs, val_h = model(inputs,val_h)\n",
        "        val_loss = criterion(val_outputs,torch.max(labels,1)[1])\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      print('Epoch:{}/{}'.format(epoch+1,n_epochs),\n",
        "            'Batch:()'.format(counter),\n",
        "            'Train Loss:{:.5f}'.format(loss.item()),\n",
        "            'Val Loss:(:.5f)'.format(np.mean(val_losses)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}